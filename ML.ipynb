{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import csv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim, dim_embed, bias=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Conv3d(dim, dim_embed, 3, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "def calc_loss(logits, target):\n",
    "    loss_func = nn.CrossEntropyLoss() # TODO: look into weights parameter\n",
    "    loss_func(logits, target)\n",
    "    return loss\n",
    "\n",
    "def calc_accuracy(logits, y): # TODO\n",
    "    y_hat = logits.max # TODO: convert logits into one-hot prediction using argmax and scatter\n",
    "    accuracy = (y_hat == y).sum() / y.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "### READ FILES\n",
    "# TODO\n",
    "folder_path = \"train/MildDemented\"  # replace with the path to your folder\n",
    "file_names_mildDemented = os.listdir(folder_path)\n",
    "\n",
    "TRAIN_SIZE = ___\n",
    "TEST_SIZE  = ___\n",
    "#NUM_LAYS   = ___\n",
    "NUM_ROWS   = ___\n",
    "NUM_COLS   = ___\n",
    "NUM_CHANS  = ___\n",
    "NUM_CLASS  = 4\n",
    "\n",
    "### SET PARAMETERS\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "SEED            = 0\n",
    "BATCH_SIZE      = 16\n",
    "LEARNING_RATE   = 0.01\n",
    "MAX_GRAD_NORM   = 2\n",
    "MAX_STEPS       = 1000\n",
    "LOG_INTERVAL    = 20\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "DIM_EMBED       = 16\n",
    "KERNEL_SIZE     = 3\n",
    "\n",
    "# LOAD CHECKPOINT INFORMATION\n",
    "CP_TIME         = ''\n",
    "CP_STEP         = 0\n",
    "\n",
    "### INITIALIZE MODEL\n",
    "model = Net(NUM_CHANS, DIM_EMBED, KERNEL_SIZE)\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "### INITIALIZE SAVE DIRECTORY\n",
    "characteristics = '{}'.format(KERNEL_SIZE)\n",
    "time_id         = time.strftime('%Y-%m-%d %H-%M-%S')\n",
    "save_dir        = os.path.join(characteristics, time_id)\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "### LOAD CHECKPOINT\n",
    "cp_path = os.path.join(characteristics, CP_TIME, '{}.pt'.format(CP_STEP))\n",
    "if os.path.isfile(cp_path):\n",
    "    print('Loading checkpoint...\\n')\n",
    "    checkpoint = torch.load(cp_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "else:\n",
    "    print('Continuing with no checkpoint... \\n')\n",
    "\n",
    "### SET LOG WRITER\n",
    "log_name = '{}.csv'.format(characteristics)\n",
    "log_path = os.path.join(save_dir, log_name)\n",
    "with open(log_path, 'w', newline='') as f:\n",
    "    header = ['Step', 'Mean_train_loss', 'train_accuracy', 'Mean_test_loss', 'test_accuracy']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "\n",
    "### TRAIN POLICY\n",
    "print('Training...\\n')\n",
    "start_time = time.time()\n",
    "for epoch in range(1, MAX_STEPS + 1):\n",
    "    loss_train = []\n",
    "    accuracy_train = []\n",
    "    #TODO: break training into batches (x and y) -> DataLoader?\n",
    "    for _ in range(TRAIN_SIZE // BATCH_SIZE):\n",
    "        logits = model(x) # TODO -> result should be shape [BATCH_SIZE, NUM_CLASS] of logits\n",
    "        target = ___ # TODO: convert y into one-hot target vectors using scatter\n",
    "        loss = calc_loss(logits, target)\n",
    "        accuracy = calc_accuracy(logits, y)\n",
    "        \n",
    "        ### APPEND LOSS AND ACCURACY\n",
    "        if epoch % LOG_INTERVAL == 0:\n",
    "            loss_train.append(loss.mean())\n",
    "            accuracy_train.append(accuracy)\n",
    "    \n",
    "        ### ACTOR UPDATE\n",
    "        model_loss = loss.mean().view(1)\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "\n",
    "    ### LOG\n",
    "    if epoch % LOG_INTERVAL == 0:\n",
    "        \n",
    "        ### SAVE CHECKPOINT\n",
    "        epoch_path = os.path.join(save_dir, \"{}.pt\".format(epoch))\n",
    "        checkpoint = {}\n",
    "        checkpoint['model'] = model.state_dict()\n",
    "        checkpoint['optimizer'] = optimizer.state_dict()\n",
    "        torch.save(checkpoint, epoch_path)\n",
    "    \n",
    "        ### START LOG\n",
    "        end_time = time.time() - start_time\n",
    "        print('Step: {}, Time: {}'.format(epoch,  time.strftime('%H:%M:%S', time.gmtime(end_time))))\n",
    "        row = [epoch]\n",
    "        \n",
    "        ### LOG TRAIN LOSS AND ACCURACY # TODO: from loss_train and accuracy_train\n",
    "        # TODO: print mean loss and accuracy\n",
    "        # TODO: add mean training loss and accuracy to log with row.extend()\n",
    "        \n",
    "        ### TEST TRAINED MODEL\n",
    "        print('Testing...')\n",
    "        with torch.no_grad():\n",
    "            # TODO: test step (same as training)\n",
    "            logits = model(x) # TODO\n",
    "            target = ___ # TODO: convert y into one-hot binary target vectors\n",
    "            loss_test = calc_loss(logits, target)\n",
    "            accuracy_test = calc_accuracy(logits, y)\n",
    "            \n",
    "        ### LOG TEST LOSS AND ACCURACY # TODO: from loss_test and accuracy_test\n",
    "        # TODO: print mean loss and accuracy\n",
    "        # TODO: add mean testing loss and accuracy to log with row.extend()\n",
    "        \n",
    "        ### LOG\n",
    "        with open(log_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
